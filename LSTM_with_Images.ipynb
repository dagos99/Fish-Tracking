{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_parts = [\n",
    "            'mouth', 'eye', 'skull', 'upper tail bone', 'lower tail bone',\n",
    "            'upper tail', 'lower tail', 'pectoral fin', 'anal fin start',\n",
    "            'anal fin mid', 'dorsal fin_base', 'dorsal fin_tip', 'stomach', 'middle'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_individuals(data_numeric, target_length=141, body_parts=None, num_individuals=8):\n",
    "    if body_parts is None:\n",
    "        body_parts = [\n",
    "            'mouth', 'eye', 'skull', 'upper tail bone', 'lower tail bone',\n",
    "            'upper tail', 'lower tail', 'pectoral fin', 'anal fin start',\n",
    "            'anal fin mid', 'dorsal fin_base', 'dorsal fin_tip', 'stomach', 'middle'\n",
    "        ]\n",
    "\n",
    "    def process_column(column, target_length):\n",
    "        result_array = np.zeros(target_length)\n",
    "        non_nan_indices = np.where(~column.isna())[0]\n",
    "        if len(non_nan_indices) > 1:\n",
    "            valid_values = column[non_nan_indices]\n",
    "            differences = np.diff(valid_values)\n",
    "            for i, diff in enumerate(differences):\n",
    "                result_array[non_nan_indices[i + 1]] = diff\n",
    "        return result_array\n",
    "\n",
    "    individual_features = {}\n",
    "\n",
    "    for individual in range(1, num_individuals + 1):\n",
    "        features_list = []\n",
    "\n",
    "        for idx, body_part in enumerate(body_parts):\n",
    "            if individual == 1 and idx == 0:\n",
    "                x_col_name = 'x'\n",
    "                y_col_name = 'y'\n",
    "            else:\n",
    "                x_col_name = f'x.{(individual - 1) * len(body_parts) + idx}'\n",
    "                y_col_name = f'y.{(individual - 1) * len(body_parts) + idx}'\n",
    "\n",
    "            if x_col_name in data_numeric.columns and y_col_name in data_numeric.columns:\n",
    "                delta_x = process_column(data_numeric[x_col_name], target_length)\n",
    "                delta_y = process_column(data_numeric[y_col_name], target_length)\n",
    "\n",
    "                if len(delta_x) > 0 and len(delta_y) > 0:\n",
    "                    speed = np.insert(np.sqrt(delta_x**2 + delta_y**2), 0, 0)\n",
    "                    direction = np.insert(np.arctan2(delta_y, delta_x), 0, 0)\n",
    "                    direction_degrees = np.degrees(direction)\n",
    "\n",
    "                    features_list.append(speed)\n",
    "                    features_list.append(direction_degrees)\n",
    "\n",
    "        if features_list:\n",
    "            individual_features[f'individual{individual}'] = pd.DataFrame(features_list).transpose()\n",
    "\n",
    "    return individual_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 227)\n",
      "individual1: 28 columns\n",
      "individual2: 28 columns\n",
      "individual3: 28 columns\n",
      "individual4: 28 columns\n",
      "individual5: 28 columns\n",
      "individual6: 28 columns\n",
      "individual7: 28 columns\n"
     ]
    }
   ],
   "source": [
    "jaime_data_numeric = pd.read_csv('CollectedData_jaime.csv', skiprows=3)\n",
    "print(jaime_data_numeric.shape)\n",
    "\n",
    "jaime_data = prepare_individuals(jaime_data_numeric, target_length=141, body_parts=body_parts, num_individuals=8)\n",
    "\n",
    "if 'individual8' in jaime_data:\n",
    "    del jaime_data['individual8']\n",
    "\n",
    "for key in jaime_data.keys():\n",
    "    print(f\"{key}: {len(jaime_data[key].columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar las etiquetas a los individuos\n",
    "jaime_labels = [0, 0, 2, 2, 1, 2, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 11s 11s/step - loss: 1.0284 - accuracy: 0.8000 - val_loss: 1.8275 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.8738 - accuracy: 0.8000 - val_loss: 1.9468 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7352 - accuracy: 0.8000 - val_loss: 2.0460 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6227 - accuracy: 1.0000 - val_loss: 2.1173 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5387 - accuracy: 1.0000 - val_loss: 2.1681 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4742 - accuracy: 1.0000 - val_loss: 2.2093 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4207 - accuracy: 1.0000 - val_loss: 2.2453 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3748 - accuracy: 1.0000 - val_loss: 2.2768 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3344 - accuracy: 1.0000 - val_loss: 2.3035 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2980 - accuracy: 1.0000 - val_loss: 2.3269 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Asignar las etiquetas a los individuos\n",
    "jaime_labels = [0, 0, 2, 2, 1, 2, 1, 1]\n",
    "\n",
    "# Asumiendo que jaime_data es un diccionario de DataFrames y jaime_labels ya está definido\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "for key, df in jaime_data.items():\n",
    "    # Convertir el DataFrame a un array 3D (samples, time steps, features)\n",
    "    individual_data = np.expand_dims(df.values, axis=0)\n",
    "    all_data.append(individual_data)\n",
    "    all_labels.append(jaime_labels[len(all_data)-1])  # Asegúrate de que jaime_labels esté en el orden correcto\n",
    "\n",
    "# Convertir listas a arrays de NumPy\n",
    "all_data = np.concatenate(all_data, axis=0)\n",
    "all_labels = to_categorical(all_labels, num_classes=3)  # Convertir etiquetas a categóricas\n",
    "\n",
    "# Dividir los datos y las etiquetas en conjuntos de entrenamiento y validación (80% - 20%)\n",
    "train_data, validation_data, train_labels, validation_labels = train_test_split(\n",
    "    all_data, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Continuar con la definición y entrenamiento del modelo como antes\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(train_data.shape[1], train_data.shape[2])),  # 50 unidades LSTM\n",
    "    Dense(3, activation='softmax')  # Capa de salida para 3 clases\n",
    "])\n",
    "model.add(Masking(mask_value=0., input_shape=(141, 56)))  # Assuming each body part has 2 features: speed and direction\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=10, validation_data=(validation_data, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 311)\n",
      "individual1: 28 columns\n",
      "individual2: 28 columns\n",
      "individual3: 28 columns\n",
      "individual4: 28 columns\n",
      "individual5: 28 columns\n",
      "individual6: 28 columns\n",
      "individual7: 28 columns\n",
      "individual8: 28 columns\n",
      "individual9: 28 columns\n",
      "individual10: 28 columns\n"
     ]
    }
   ],
   "source": [
    "katia_data_numeric = pd.read_csv('new_CollectedData_katia.csv', skiprows=3)\n",
    "print(katia_data_numeric.shape)\n",
    "\n",
    "katia_data = prepare_individuals(katia_data_numeric, target_length=56, body_parts=body_parts, num_individuals=11)\n",
    "    \n",
    "if 'individual11' in katia_data:\n",
    "    del katia_data['individual11']\n",
    "    \n",
    "for key in katia_data.keys():\n",
    "    print(f\"{key}: {len(katia_data[key].columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 526ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Prepare test data with padding\n",
    "test_data_padded = []\n",
    "\n",
    "for key, df in katia_data.items():\n",
    "    # Convert the DataFrame to a 3D array (samples, time steps, features)\n",
    "    individual_data = np.expand_dims(df.values, axis=0)\n",
    "    # Pad sequences to match the model's expected input shape (142 time steps)\n",
    "    individual_data_padded = pad_sequences(individual_data, maxlen=142, dtype='float32', padding='post', truncating='post', value=0.0)\n",
    "    test_data_padded.append(individual_data_padded)\n",
    "\n",
    "# Convert list to a NumPy array\n",
    "test_data_padded = np.concatenate(test_data_padded, axis=0)\n",
    "\n",
    "# Make predictions with padded test data\n",
    "predictions = model.predict(test_data_padded)\n",
    "\n",
    "# Optionally, convert predictions to labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
