{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-5sIy6gU-cC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBGp5mh7EBlP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTWDy96ATR-m",
        "outputId": "276456a2-2454-4864-ae59-782e0fde7414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYfndN9TEBlR"
      },
      "outputs": [],
      "source": [
        "body_parts = [\n",
        "            'mouth', 'eye', 'skull', 'upper tail bone', 'lower tail bone',\n",
        "            'upper tail', 'lower tail', 'pectoral fin', 'anal fin start',\n",
        "            'anal fin mid', 'dorsal fin_base', 'dorsal fin_tip', 'stomach', 'middle'\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XEx2Ka-EBlS"
      },
      "outputs": [],
      "source": [
        "def prepare_individuals(data_numeric, target_length=141, body_parts=None, num_individuals=8):\n",
        "    # Define body parts if not provided\n",
        "    if body_parts is None:\n",
        "        body_parts = [\n",
        "            'mouth', 'eye', 'skull', 'upper tail bone', 'lower tail bone',\n",
        "            'upper tail', 'lower tail', 'pectoral fin', 'anal fin start',\n",
        "            'anal fin mid', 'dorsal fin_base', 'dorsal fin_tip', 'stomach', 'middle'\n",
        "        ]\n",
        "\n",
        "    def process_column(column, target_length):\n",
        "        result_array = np.zeros(target_length)\n",
        "        non_nan_indices = np.where(~column.isna())[0]\n",
        "        if len(non_nan_indices) > 1:\n",
        "            valid_values = column[non_nan_indices]\n",
        "            differences = np.diff(valid_values)\n",
        "            for i, diff in enumerate(differences):\n",
        "                result_array[non_nan_indices[i + 1]] = diff\n",
        "        return result_array\n",
        "\n",
        "    # Dictionary to keep DataFrames for each individual\n",
        "    individual_features = {}\n",
        "\n",
        "    for individual in range(1, num_individuals + 1):\n",
        "        features_list = []  # List to accumulate the features of this individual\n",
        "\n",
        "        for idx, body_part in enumerate(body_parts):\n",
        "            # Handle the first individual and the first body part\n",
        "            if individual == 1 and idx == 0:\n",
        "                x_col_name = 'x'\n",
        "                y_col_name = 'y'\n",
        "            else:\n",
        "                x_col_name = f'x.{(individual - 1) * len(body_parts) + idx}'\n",
        "                y_col_name = f'y.{(individual - 1) * len(body_parts) + idx}'\n",
        "\n",
        "            if x_col_name in data_numeric.columns and y_col_name in data_numeric.columns:\n",
        "                delta_x = process_column(data_numeric[x_col_name], target_length)\n",
        "                delta_y = process_column(data_numeric[y_col_name], target_length)\n",
        "\n",
        "                if len(delta_x) > 0 and len(delta_y) > 0:\n",
        "                    speed = np.insert(np.sqrt(delta_x**2 + delta_y**2), 0, 0)\n",
        "                    direction = np.insert(np.arctan2(delta_y, delta_x), 0, 0)\n",
        "                    direction_degrees = np.degrees(direction)\n",
        "\n",
        "                    # Add the features to the list\n",
        "                    features_list.append(speed)\n",
        "                    features_list.append(direction_degrees)\n",
        "\n",
        "        # Create a DataFrame for this individual from the list of features\n",
        "        if features_list:\n",
        "            individual_features[f'individual{individual}'] = pd.DataFrame(features_list).transpose()\n",
        "\n",
        "    return individual_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mpTonP_EBlS",
        "outputId": "c0318e09-94f6-4a42-8ee4-2dd29f03741a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(141, 227)\n",
            "dict_keys(['individual1', 'individual2', 'individual3', 'individual4', 'individual5', 'individual6', 'individual7', 'individual8'])\n",
            "individual1: 28 columns\n",
            "individual2: 28 columns\n",
            "individual3: 28 columns\n",
            "individual4: 28 columns\n",
            "individual5: 28 columns\n",
            "individual6: 28 columns\n",
            "individual7: 28 columns\n"
          ]
        }
      ],
      "source": [
        "data_numeric = pd.read_csv('TrainDataFishVideo.csv', skiprows=3)\n",
        "print(data_numeric.shape)\n",
        "\n",
        "train_data = prepare_individuals(data_numeric, target_length=141, body_parts=body_parts, num_individuals=8)\n",
        "print(train_data.keys())\n",
        "if 'individual8' in train_data:\n",
        "    del train_data['individual8']\n",
        "for key in train_data.keys():\n",
        "    print(f\"{key}: {len(train_data[key].columns)} columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE5Kg1CBEBlT",
        "outputId": "de399846-55a2-4098-b6d5-6aefcfcc0ba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1411 - accuracy: 0.5000 - val_loss: 1.9425 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 1.0058 - accuracy: 0.6667 - val_loss: 1.9742 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.9127 - accuracy: 0.8333 - val_loss: 1.9884 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.8371 - accuracy: 0.8333 - val_loss: 1.9969 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7700 - accuracy: 0.8333 - val_loss: 2.0064 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7087 - accuracy: 0.8333 - val_loss: 2.0218 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6517 - accuracy: 0.8333 - val_loss: 2.0429 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.5969 - accuracy: 0.8333 - val_loss: 2.0671 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.5419 - accuracy: 0.8333 - val_loss: 2.0923 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.4863 - accuracy: 0.8333 - val_loss: 2.1166 - val_accuracy: 0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Asignar las etiquetas a los individuos\n",
        "jaime_labels = [0, 0, 2, 2, 1, 2, 1]\n",
        "jaime_labels = [0, 0, 2, 2, 1, 2, 1]\n",
        "\n",
        "# Preparar datos de entrenamiento y validación\n",
        "train_data = []\n",
        "train_labels = []\n",
        "validation_data = []\n",
        "validation_labels = []\n",
        "\n",
        "for i, (key, df) in enumerate(train_data.items()):\n",
        "    # Convertir el DataFrame a un array 3D (samples, time steps, features)\n",
        "    individual_data = np.expand_dims(df.values, axis=0)\n",
        "\n",
        "    if i < 6:  # Primeros 6 individuos para entrenamiento\n",
        "        train_data.append(individual_data)\n",
        "        train_labels.append(jaime_labels[i])\n",
        "    else:  # Últimos 2 individuos para validación\n",
        "        validation_data.append(individual_data)\n",
        "        validation_labels.append(jaime_labels[i])\n",
        "\n",
        "# Convertir listas a arrays de NumPy\n",
        "train_data = np.concatenate(train_data, axis=0)\n",
        "train_labels = to_categorical(train_labels, num_classes=3)\n",
        "validation_data = np.concatenate(validation_data, axis=0)\n",
        "validation_labels = to_categorical(validation_labels, num_classes=3)\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(50, input_shape=(train_data.shape[1], train_data.shape[2])),  # 50 unidades LSTM\n",
        "    Dense(3, activation='softmax')  # Capa de salida para 3 clases\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_data=(validation_data, validation_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0fPD2K1ysAG",
        "outputId": "41f5b0e8-0169-4413-9a10-8ab164b9db37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.1040 - accuracy: 0.4286\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0124 - accuracy: 0.4286\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.9258 - accuracy: 0.5714\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.8447 - accuracy: 0.7143\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.7724 - accuracy: 0.8571\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7080 - accuracy: 0.8571\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6503 - accuracy: 0.8571\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5997 - accuracy: 0.8571\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5596 - accuracy: 0.8571\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5279 - accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Assuming jaime_data is your full dataset and jaime_labels are the corresponding labels\n",
        "\n",
        "# Prepare full training data and labels\n",
        "full_train_data = []\n",
        "full_train_labels = []\n",
        "\n",
        "for i, (key, df) in enumerate(train_data.items()):\n",
        "    # Convert each DataFrame to a 3D array (samples, time steps, features)\n",
        "    individual_data = np.expand_dims(df.values, axis=0)\n",
        "    full_train_data.append(individual_data)\n",
        "    full_train_labels.append(jaime_labels[i])\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "full_train_data = np.concatenate(full_train_data, axis=0)\n",
        "full_train_labels = np.array(full_train_labels)\n",
        "\n",
        "# Convert labels to categorical (one-hot encoding)\n",
        "num_classes = len(np.unique(full_train_labels))  # assuming this is how many unique labels you have\n",
        "full_train_labels = to_categorical(full_train_labels, num_classes=num_classes)\n",
        "\n",
        "# Define your LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(30, input_shape=(full_train_data.shape[1], full_train_data.shape[2])),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the full data\n",
        "history = model.fit(full_train_data, full_train_labels, epochs=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okVcp8NK35JV",
        "outputId": "eff44c74-7e43-4b9e-a144-93ca9486d485"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7, 142, 28)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3br3lPIxU-Yj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# base_path = '/content/drive/MyDrive/Fish2'\n",
        "base_path = 'Fish2'\n",
        "\n",
        "# Function to preprocess an image\n",
        "def preprocess_image(image_path, target_size=(224, 224)):\n",
        "    with Image.open(image_path) as img:\n",
        "        img = img.convert('RGB')  # Convert image to RGB\n",
        "        img = img.resize(target_size)\n",
        "        image_array = np.array(img)\n",
        "        image_array = image_array / 255.0\n",
        "    return image_array\n",
        "\n",
        "# Function to create sequences with proper padding for intermittent visibility\n",
        "def create_custom_padded_sequences(base_path, appearance_times, max_sequence_length):\n",
        "    sequences = {}  # Dictionary to hold image sequences for each individual\n",
        "\n",
        "    # Sort the folder names numerically\n",
        "    sorted_folders = sorted(os.listdir(base_path), key=lambda x: int(re.search(r'\\d+', x).group()))\n",
        "\n",
        "    # Iterate over the sorted folders and the images within them\n",
        "    for folder_name in sorted_folders:\n",
        "        folder_path = os.path.join(base_path, folder_name)\n",
        "        if os.path.isdir(folder_path):\n",
        "            # List to hold preprocessed images for the current folder\n",
        "            images = [preprocess_image(os.path.join(folder_path, img_name))\n",
        "                      for img_name in sorted(os.listdir(folder_path), key=lambda x: int(re.search(r'\\d+', x).group()))]\n",
        "            sequences[folder_name] = images\n",
        "\n",
        "    # Apply custom padding based on appearance times\n",
        "    padded_sequences = []\n",
        "    for folder_name, images in sequences.items():\n",
        "        sequence_padding = []\n",
        "        image_index = 0  # Index to keep track of the current image\n",
        "        intervals = appearance_times.get(folder_name, [(0, max_sequence_length)])\n",
        "\n",
        "        for frame_number in range(max_sequence_length):\n",
        "            # Check if the frame number is within any visibility interval\n",
        "            if any(start <= frame_number < end for start, end in intervals):\n",
        "                if image_index < len(images):  # Check if there are still images left to append\n",
        "                    sequence_padding.append(images[image_index])  # add the next image\n",
        "                    image_index += 1\n",
        "                else:\n",
        "                    # If no images are left, append a blank image\n",
        "                    sequence_padding.append(np.zeros((224, 224, 3)))\n",
        "            else:\n",
        "                # If not visible, append a blank image\n",
        "                sequence_padding.append(np.zeros((224, 224, 3)))\n",
        "        padded_sequences.append(sequence_padding)\n",
        "\n",
        "\n",
        "    return padded_sequences\n",
        "\n",
        "# Define appearance times including all visible intervals\n",
        "# Define appearance times including all visible intervals\n",
        "appearance_times = {\n",
        "    '0' : [(0,136)],\n",
        "    '1': (0, 139),\n",
        "    '2-21-31-132': [(0, 21), (31, 132)],\n",
        "    '3': (0, 141),\n",
        "    '4-060': (60, 141),\n",
        "    '5-117': (117, 141),\n",
        "    '6-122': [(122, 141)]\n",
        "}\n",
        "\n",
        "max_sequence_length = 141  # Or the length of your video sequence\n",
        "padded_image_sequences = create_custom_padded_sequences(base_path, appearance_times, max_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyh24ACzU-WN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Conv2D, MaxPooling2D, Flatten, Masking\n",
        "\n",
        "\n",
        "def create_cnn_model():\n",
        "    # Load ResNet50 with pre-trained weights, without the top classification layer\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # Freeze the layers of the base_model\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Add a Flatten layer or GlobalAveragePooling layer here depending on your needs\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)  # or you can use GlobalAveragePooling2D()\n",
        "\n",
        "    # Create the full model\n",
        "    model = Model(inputs=base_model.input, outputs=x)\n",
        "    return model\n",
        "\n",
        "# Define LSTM model that will take the CNN features as input\n",
        "def create_lstm_model():\n",
        "    cnn_model = create_cnn_model()\n",
        "    model = Sequential()\n",
        "    # model.add(Masking(mask_value=0., input_shape=(141, 224, 224, 3)))\n",
        "    model.add(TimeDistributed(cnn_model, input_shape=(max_sequence_length, 224, 224, 3)))\n",
        "    model.add(LSTM(units=50, return_sequences=False))\n",
        "    # add more layers as needed...\n",
        "    model.add(Dense(units=3, activation='softmax'))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1AgyRU4zRQA",
        "outputId": "1836d054-5e05-4848-b66b-b18abc1dc81d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7, 141, 224, 224, 3)\n",
            "(7,)\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 53s 7s/step - loss: 1.1000 - accuracy: 0.2857\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 447ms/step - loss: 1.6718 - accuracy: 0.4286\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 462ms/step - loss: 1.4235 - accuracy: 0.4286\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 449ms/step - loss: 1.2465 - accuracy: 0.4286\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 449ms/step - loss: 1.2260 - accuracy: 0.4286\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 452ms/step - loss: 1.1968 - accuracy: 0.4286\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 464ms/step - loss: 1.1548 - accuracy: 0.4286\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 442ms/step - loss: 1.1316 - accuracy: 0.4286\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 426ms/step - loss: 1.1029 - accuracy: 0.4286\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 432ms/step - loss: 1.0837 - accuracy: 0.4286\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ae27c11b280>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "jaime_labels = [0, 0, 2, 2, 1, 2, 1]\n",
        "\n",
        "# Assume padded_image_sequences is a list of NumPy arrays (your image data)\n",
        "# and jaime_labels is a list or array of labels\n",
        "class AdamW(Adam):\n",
        "    def __init__(self, weight_decay, **kwargs):\n",
        "        super(AdamW, self).__init__(**kwargs)\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
        "        var_dtype = var.dtype.base_dtype\n",
        "        lr_t = self._decayed_lr(var_dtype)  # handle learning rate decay\n",
        "        wd = self.weight_decay * lr_t\n",
        "        var.assign_sub(wd * var, use_locking=self._use_locking)\n",
        "        return super(AdamW, self)._resource_apply_dense(grad, var, apply_state)\n",
        "\n",
        "# Convert the list of labels to a numpy array if they aren't already\n",
        "jaime_labels_array = np.array(jaime_labels)\n",
        "\n",
        "# If padded_image_sequences is a list of arrays, convert it to a single NumPy array\n",
        "padded_image_sequences_array = np.stack(padded_image_sequences)\n",
        "print(padded_image_sequences_array.shape)  # Should output something like (num_samples, sequence_length, height, width, channels)\n",
        "print(jaime_labels_array.shape)  # Should output (num_samples,)\n",
        "\n",
        "# Now padded_image_sequences_array and jaime_labels_array are your full dataset and labels\n",
        "\n",
        "# Initialize AdamW optimizer with weight decay\n",
        "adam_optimizer_custom = AdamW(weight_decay=0.01, learning_rate=0.0001)\n",
        "\n",
        "# Compile the hybrid CNN-LSTM model\n",
        "hybrid_model = create_lstm_model()\n",
        "hybrid_model.compile(optimizer=adam_optimizer_custom, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model using the full dataset\n",
        "hybrid_model.fit(\n",
        "    padded_image_sequences_array,\n",
        "    jaime_labels_array,\n",
        "    batch_size = 6 ,\n",
        "    epochs=10  # or however many epochs you wish to train for\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOQJUCPEeipN"
      },
      "source": [
        "# Test ensemble learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdpChpymwmGf"
      },
      "source": [
        "preprocessing test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1DwnubuU-Gy",
        "outputId": "268311bf-ecfa-4d16-c82e-d481aff9a5a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "Folder: 0, Intervals: [(0, 57)]\n",
            "1\n",
            "Folder: 1, Intervals: [(0, 57)]\n",
            "2\n",
            "Folder: 2, Intervals: [(0, 57)]\n",
            "3\n",
            "Folder: 3, Intervals: [(0, 57)]\n",
            "4\n",
            "Folder: 4, Intervals: [(0, 37)]\n",
            "5\n",
            "Folder: 5, Intervals: [(0, 57)]\n",
            "6\n",
            "Folder: 6, Intervals: [(0, 57)]\n",
            "22\n",
            "Folder: 22, Intervals: [(24, 57)]\n",
            "25\n",
            "Folder: 25, Intervals: [(27, 57)]\n",
            "40\n",
            "Folder: 40, Intervals: [(40, 57)]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# base_path = '/content/drive/MyDrive/Fish'\n",
        "base_path = 'Fish'\n",
        "\n",
        "# Function to preprocess an image\n",
        "def preprocess_image(image_path, target_size=(224, 224)):\n",
        "    with Image.open(image_path) as img:\n",
        "        img = img.convert('RGB')  # Convert image to RGB\n",
        "        img = img.resize(target_size)\n",
        "        image_array = np.array(img)\n",
        "        image_array = image_array / 255.0\n",
        "    return image_array\n",
        "\n",
        "# Function to create sequences with proper padding for intermittent visibility\n",
        "def create_custom_padded_sequences(base_path, appearance_times, max_sequence_length):\n",
        "    sequences = {}  # Dictionary to hold image sequences for each individual\n",
        "\n",
        "    # Sort the folder names numerically\n",
        "    sorted_folders = sorted(os.listdir(base_path), key=lambda x: int(re.search(r'\\d+', x).group()))\n",
        "\n",
        "    # Iterate over the sorted folders and the images within them\n",
        "    for folder_name in sorted_folders:\n",
        "        folder_path = os.path.join(base_path, folder_name)\n",
        "        if os.path.isdir(folder_path):\n",
        "            # List to hold preprocessed images for the current folder\n",
        "            images = [preprocess_image(os.path.join(folder_path, img_name))\n",
        "                      for img_name in sorted(os.listdir(folder_path), key=lambda x: int(re.search(r'\\d+', x).group()))]\n",
        "            sequences[folder_name] = images\n",
        "\n",
        "    # Apply custom padding based on appearance times\n",
        "    padded_sequences = []\n",
        "    for folder_name, images in sequences.items():\n",
        "        print(folder_name)\n",
        "        sequence_padding = []\n",
        "        image_index = 0  # Index to keep track of the current image\n",
        "        intervals = appearance_times.get(folder_name, [(0, max_sequence_length)])\n",
        "        print(f\"Folder: {folder_name}, Intervals: {intervals}\")  # Add this line to check the intervals format\n",
        "\n",
        "        for frame_number in range(max_sequence_length):\n",
        "            # Check if the frame number is within any visibility interval\n",
        "            if any(start <= frame_number < end for start, end in intervals):\n",
        "                if image_index < len(images):  # Check if there are still images left to append\n",
        "                    sequence_padding.append(images[image_index])  # add the next image\n",
        "                    image_index += 1\n",
        "                else:\n",
        "                    # If no images are left, append a blank image\n",
        "                    sequence_padding.append(np.zeros((224, 224, 3)))\n",
        "            else:\n",
        "                # If not visible, append a blank image\n",
        "                sequence_padding.append(np.zeros((224, 224, 3)))\n",
        "        padded_sequences.append(sequence_padding)\n",
        "\n",
        "\n",
        "    return padded_sequences\n",
        "\n",
        "# Define appearance times including all visible intervals\n",
        "# Define appearance times including all visible intervals\n",
        "appearance_times = {\n",
        "    '0' :[(0,57)],\n",
        "    '1': [(0, 57)],\n",
        "    '2': [(0, 57)],\n",
        "    '3': [(0, 57)],\n",
        "    '4': [(0, 37)],\n",
        "    '5': [(0, 57)],\n",
        "    '6': [(0, 57)],\n",
        "    '22':[(24, 57)],\n",
        "    '25': [(27, 57)],\n",
        "    '40': [(40, 57)]\n",
        "}\n",
        "\n",
        "max_sequence_length = 141  # Or the length of your video sequence\n",
        "X_test_images = create_custom_padded_sequences(base_path, appearance_times, max_sequence_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac25KaCWf9rN",
        "outputId": "c8199c4d-1bf2-4d75-8181-70cc5f307988"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(56, 311)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# test_data_numeric = pd.read_csv('/content/drive/MyDrive/CollectedData_katia.csv', skiprows=3)\n",
        "test_data_numeric = pd.read_csv('TrainDataFishVideo.csv', skiprows=3)\n",
        "\n",
        "print(test_data_numeric.shape)\n",
        "\n",
        "test_data = prepare_individuals(test_data_numeric, target_length=56, body_parts=body_parts, num_individuals=10)\n",
        "\n",
        "\n",
        "# Prepare test data with padding\n",
        "test_data_padded = []\n",
        "\n",
        "for key, df in test_data.items():\n",
        "    # Convert the DataFrame to a 3D array (samples, time steps, features)\n",
        "    individual_data = np.expand_dims(df.values, axis=0)\n",
        "    # Pad sequences to match the model's expected input shape (142 time steps)\n",
        "    individual_data_padded = pad_sequences(individual_data, maxlen=142, dtype='float32', padding='post', truncating='post', value=0.0)\n",
        "    test_data_padded.append(individual_data_padded)\n",
        "\n",
        "# Convert list to a NumPy array\n",
        "test_data_padded = np.concatenate(test_data_padded, axis=0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5ifMH_T4IUD",
        "outputId": "87be9abc-c709-474a-804a-6ba186e99ca9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10, 142, 28)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data_padded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S7gmv7rU6sF",
        "outputId": "dcb9debc-b01d-46c0-a66f-9959bb615251"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 376ms/step\n",
            "[[0.3258123  0.3230889  0.35109872]\n",
            " [0.32581228 0.32308882 0.35109887]\n",
            " [0.32581225 0.3230888  0.3510989 ]\n",
            " [0.32581234 0.323089   0.35109863]\n",
            " [0.32581225 0.32308882 0.35109892]\n",
            " [0.32581237 0.32308894 0.35109866]\n",
            " [0.32581225 0.32308877 0.35109898]\n",
            " [0.3258124  0.32308927 0.35109836]\n",
            " [0.32581228 0.3230891  0.3510986 ]\n",
            " [0.3258123  0.32308897 0.3510987 ]]\n",
            "(10, 3)\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "[[0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]]\n",
            "(10, 3)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def ensemble_predictions(model1, model2, data1, data2, weights=[0.5, 0.5]):\n",
        "    # Ensure weights sum to 1\n",
        "    weights = np.array(weights) / sum(weights)\n",
        "\n",
        "    # Predict with each model\n",
        "    preds1 = model1.predict(data1)  # Assuming this returns class probabilities for the numerical data model\n",
        "    print(preds1)\n",
        "    print(preds1.shape)\n",
        "    X_test_array = np.stack(data2)\n",
        "    batch_size = 1  # Adjust the batch size to fit your system's memory\n",
        "    num_samples = X_test_array.shape[0]\n",
        "\n",
        "    preds2 = []\n",
        "    for i in range(0, X_test_array.shape[0], batch_size):\n",
        "        end_index = i + batch_size\n",
        "        # Ensure we don't go past the end of the array on the last batch\n",
        "        if end_index > X_test_array.shape[0]:\n",
        "            end_index = X_test_array.shape[0]\n",
        "        batch_predictions = hybrid_model.predict(X_test_array[i:end_index])\n",
        "        preds2.append(batch_predictions)\n",
        "\n",
        "    # Concatenate all batch predictions into a single array\n",
        "    preds2 = np.vstack(preds2)  # Use vstack to stack arrays vertically\n",
        "    print(preds2)\n",
        "    print(preds2.shape)\n",
        "\n",
        "\n",
        "    # Combine predictions\n",
        "    weights = np.array(weights)\n",
        "\n",
        "    combined_preds = (weights[0] * preds1) + (weights[1] * preds2)\n",
        "\n",
        "    # Final prediction\n",
        "    final_preds = np.argmax(combined_preds, axis=1)\n",
        "\n",
        "    return final_preds\n",
        "ensemble_predictions(model, hybrid_model,test_data_padded , X_test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XusNjjLkld0V"
      },
      "outputs": [],
      "source": [
        "# true_labels = [0, 1, 0, 2, 1, 3, 3,1,1]  # Assuming you have a corresponding y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqNRspLu5g67",
        "outputId": "5fb8ad1e-a34e-4132-db08-8618cff13328"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 197ms/step\n",
            "[[0.3258123  0.3230889  0.35109872]\n",
            " [0.32581228 0.32308882 0.35109887]\n",
            " [0.32581225 0.3230888  0.3510989 ]\n",
            " [0.32581234 0.323089   0.35109863]\n",
            " [0.32581225 0.32308882 0.35109892]\n",
            " [0.32581237 0.32308894 0.35109866]\n",
            " [0.32581225 0.32308877 0.35109898]\n",
            " [0.3258124  0.32308927 0.35109836]\n",
            " [0.32581228 0.3230891  0.3510986 ]\n",
            " [0.3258123  0.32308897 0.3510987 ]]\n",
            "(10, 3)\n",
            "1/1 [==============================] - 0s 270ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 308ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "[[0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]\n",
            " [0.2806388  0.32924953 0.3901117 ]]\n",
            "(10, 3)\n",
            "Model accuracy: 10.00%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming you have a variable `true_labels` which contains the true class indices\n",
        "true_labels = np.array([0, 1, 0, 2, 1, 3, 3,1,1,3])\n",
        "\n",
        "# Let's say `padded_image_sequences_array` is your test data\n",
        "# You would get predictions from your model like this:\n",
        "predicted_classes = ensemble_predictions(model, hybrid_model,test_data_padded , X_test_images)\n",
        "\n",
        "# Now you have the predicted class indices, you can compare them with the true labels\n",
        "accuracy = accuracy_score(true_labels, predicted_classes)\n",
        "\n",
        "# Print out the accuracy\n",
        "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
